{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f4fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46aa3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197f340",
   "metadata": {},
   "source": [
    "1. create dataframe.\n",
    "2. how to insert new column in dataframe.\n",
    "3. how to merge or join two dataframe.\n",
    "4. How to concat dataframe.\n",
    "5. What will be the out put of inner and outer join.\n",
    "6. How to select the recors based on praority.\n",
    "7. how to Canvert Columns to Rows.  \n",
    "8. how to split string into columns.\n",
    "9. how to find diff in current record and privius record.\n",
    "10. customer['cid','name','city','state'] order['oid','cid','itam','price'] find the custimer from 'Mumbai' who has minimam orders.\n",
    "11. RDDs vs. Dataframes in PySpark.\n",
    "12. What are workers, executors, cores in Spark Standalone cluster?\n",
    "13. find third higest salary\n",
    "16. how can we add a column in panada dataframe\n",
    "17. what are the task you have done in your machine learning project.\n",
    "18. in linear regression what do you mean by gradient decent? what do you mean by global minima.\n",
    "19. what is overfitting and underfitting?\n",
    "20. in logestic regression what is the default threshold value?\n",
    "21. what do you mean by confussion metrix?\n",
    "22. what is precession and recall?\n",
    "23. what is AUC and RUC curve?\n",
    "34. discribe the ways to combine 2 dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd97921a",
   "metadata": {},
   "source": [
    "### 1. create dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b349f2fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>d*100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  d*100\n",
       "0    10    110\n",
       "1    20    120\n",
       "2    30    130"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multidymentional array\n",
    "# data = [['tom', 10], ['nick', 15], ['juli', 14]]\n",
    "# df = pd.DataFrame(data, columns = ['Name', 'Age'])\n",
    "# df\n",
    "\n",
    "# Creating DataFrame from dict of narray/lists\n",
    "# data = {'Name':['Tom', 'nick', 'krish', 'jack'],'Age':[20, 21, 19, 18]}\n",
    "# df = pd.DataFrame(data)\n",
    "# df\n",
    "\n",
    "# Creates a indexes DataFrame using arrays.\n",
    "# data = {'Name':['Tom', 'nick', 'krish', 'jack'],'Age':[20, 21, 19, 18]}\n",
    "# df = pd.DataFrame(data, index =['rank1','rank2','rank3','rank4'])\n",
    "# df\n",
    "\n",
    "# Creating Dataframe from list of dicts\n",
    "# data = [{'a': 1, 'b': 2, 'c':3},{'a':10, 'b': 20, 'c': 30}]\n",
    "# df = pd.DataFrame(data)\n",
    "# df\n",
    "\n",
    "# Creating DataFrame using zip() function.\n",
    "# Name = ['tom', 'krish', 'nick', 'juli'] \n",
    "# Age = [25, 30, 26, 22] \n",
    "# list_of_tuples = list(zip(Name, Age)) \n",
    "# print(list_of_tuples)  \n",
    "# df = pd.DataFrame(list_of_tuples,columns = ['Name', 'Age']) \n",
    "# df \n",
    "\n",
    "# Creating DataFrame from Dicts of series.\n",
    "# d = {'one' : pd.Series([10, 20, 30, 40],index =['a', 'b', 'c', 'd']),\n",
    "#       'two' : pd.Series([10, 20, 30, 40],index =['a', 'b', 'c', 'd'])}\n",
    "# df = pd.DataFrame(d)\n",
    "# df\n",
    "\n",
    "# Creating DataFrame from Dicts of series.\n",
    "d = [10,20,30]\n",
    "ds = [110,120,130]\n",
    "df = pd.DataFrame({'data':d,'d*100':ds})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b50436c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.createDataFrame([['1a','1b','1c'],['2a','2b','2c'],['3a','3b','3c']], schema='a string, b string, c string')\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cde7f1",
   "metadata": {},
   "source": [
    "### 2. how to insert new column in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb06f15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-05bc92b4abd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1d'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2d'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'3d'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['d']=['1d','2d','3d']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e635e",
   "metadata": {},
   "source": [
    "### 3. how to merge or join two dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c866c671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K0</td>\n",
       "      <td>A0</td>\n",
       "      <td>B0</td>\n",
       "      <td>C0</td>\n",
       "      <td>D0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K1</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>C1</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K2</td>\n",
       "      <td>A2</td>\n",
       "      <td>B2</td>\n",
       "      <td>C2</td>\n",
       "      <td>D2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K3</td>\n",
       "      <td>A3</td>\n",
       "      <td>B3</td>\n",
       "      <td>C3</td>\n",
       "      <td>D3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key   A   B   C   D\n",
       "0  K0  A0  B0  C0  D0\n",
       "1  K1  A1  B1  C1  D1\n",
       "2  K2  A2  B2  C2  D2\n",
       "3  K3  A3  B3  C3  D3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "                     'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                     'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "   \n",
    "right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "                          'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                          'D': ['D0', 'D1', 'D2', 'D3']}) \n",
    "\n",
    "pd.merge(left,right,how='inner',on='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2840725d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>K0</th>\n",
       "      <td>A0</td>\n",
       "      <td>B0</td>\n",
       "      <td>C0</td>\n",
       "      <td>D0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K1</th>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K2</th>\n",
       "      <td>A2</td>\n",
       "      <td>B2</td>\n",
       "      <td>C2</td>\n",
       "      <td>D2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C3</td>\n",
       "      <td>D3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A    B    C    D\n",
       "K0   A0   B0   C0   D0\n",
       "K1   A1   B1  NaN  NaN\n",
       "K2   A2   B2   C2   D2\n",
       "K3  NaN  NaN   C3   D3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n",
    "                     'B': ['B0', 'B1', 'B2']},\n",
    "                      index=['K0', 'K1', 'K2']) \n",
    "\n",
    "right = pd.DataFrame({'C': ['C0', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D2', 'D3']},\n",
    "                      index=['K0', 'K2', 'K3'])\n",
    "#left.join(right)\n",
    "left.join(right, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1524681",
   "metadata": {},
   "source": [
    "### 4. How to concat dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17ef561c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0</td>\n",
       "      <td>B0</td>\n",
       "      <td>C0</td>\n",
       "      <td>D0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>C1</td>\n",
       "      <td>D1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2</td>\n",
       "      <td>B2</td>\n",
       "      <td>C2</td>\n",
       "      <td>D2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3</td>\n",
       "      <td>B3</td>\n",
       "      <td>C3</td>\n",
       "      <td>D3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A4</td>\n",
       "      <td>B4</td>\n",
       "      <td>C4</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A5</td>\n",
       "      <td>B5</td>\n",
       "      <td>C5</td>\n",
       "      <td>D5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A6</td>\n",
       "      <td>B6</td>\n",
       "      <td>C6</td>\n",
       "      <td>D6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A7</td>\n",
       "      <td>B7</td>\n",
       "      <td>C7</td>\n",
       "      <td>D7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C    D    A    B    C    D\n",
       "0   A0   B0   C0   D0  NaN  NaN  NaN  NaN\n",
       "1   A1   B1   C1   D1  NaN  NaN  NaN  NaN\n",
       "2   A2   B2   C2   D2  NaN  NaN  NaN  NaN\n",
       "3   A3   B3   C3   D3  NaN  NaN  NaN  NaN\n",
       "4  NaN  NaN  NaN  NaN   A4   B4   C4   D4\n",
       "5  NaN  NaN  NaN  NaN   A5   B5   C5   D5\n",
       "6  NaN  NaN  NaN  NaN   A6   B6   C6   D6\n",
       "7  NaN  NaN  NaN  NaN   A7   B7   C7   D7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                        'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                        'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                        'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                        index=[0, 1, 2, 3])\n",
    "df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "                        'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "                        'C': ['C4', 'C5', 'C6', 'C7'],\n",
    "                        'D': ['D4', 'D5', 'D6', 'D7']},\n",
    "                         index=[4, 5, 6, 7]) \n",
    "df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],\n",
    "                        'B': ['B8', 'B9', 'B10', 'B11'],\n",
    "                        'C': ['C8', 'C9', 'C10', 'C11'],\n",
    "                        'D': ['D8', 'D9', 'D10', 'D11']},\n",
    "                        index=[8, 9, 10, 11])\n",
    "#pd.concat([df1,df2,df3])\n",
    "#pd.concat([df1,df2])\n",
    "pd.concat([df1,df2],axis=1)\n",
    "#df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec9b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c60f6ce0",
   "metadata": {},
   "source": [
    "### 5. What will be the out put of inner and outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6f2b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- USING PANDAS-----\n",
      "   id name_x name_y\n",
      "0   1      A      C\n",
      "1   1      A      D\n",
      "2   1      B      C\n",
      "3   1      B      D\n",
      "----- USING PYSPARK-----\n",
      "+---+----+---+----+\n",
      "| id|name| id|name|\n",
      "+---+----+---+----+\n",
      "|  1|   A|  1|   D|\n",
      "|  1|   A|  1|   C|\n",
      "|  1|   B|  1|   D|\n",
      "|  1|   B|  1|   C|\n",
      "+---+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input:\n",
    "#   left               right                      \n",
    "#  id,name            id,name                  \n",
    "#  1,  A              1,  C                        \n",
    "#  1,  B              1,  D                        \n",
    "#  2,  E              3,  F                         \n",
    "\n",
    "\n",
    "# USING SQL\n",
    "# select l.id,l.fname,r.id,r.fname from tb_left as l INNER JOIN tb_right as r on l.id=r.id;\n",
    "# Output:\n",
    "# 1\tB\t1\tC\n",
    "# 1\tA\t1\tC\n",
    "# 1\tB\t1\tD\n",
    "# 1\tA\t1\tD\n",
    "\n",
    "# SELECT * FROM tb_left as l left join tb_right as r on l.id=r.id\n",
    "# UNION\n",
    "# SELECT * FROM tb_left as l right join tb_right as r on l.id=r.id;\n",
    "# 1\tA\t1\tD\n",
    "# 1\tA\t1\tC\n",
    "# 1\tB\t1\tD\n",
    "# 1\tB\t1\tC\n",
    "# 2\tE\t\t\n",
    "# \t\t3\tF\n",
    " \n",
    "print(\"----- USING PANDAS-----\")\n",
    "left = pd.read_csv('left.csv')\n",
    "right = pd.read_csv('right.csv')\n",
    "\n",
    "print(pd.merge(left,right,how='inner',on='id'))\n",
    "# print(pd.merge(left,right,how='outer',on='id'))\n",
    "# print(pd.merge(left,right,how='left',on='id'))\n",
    "# print(pd.merge(left,right,how='cross'))\n",
    "\n",
    "print(\"----- USING PYSPARK-----\")\n",
    "left = spark.read.csv('left.csv',header=True,inferSchema=True)\n",
    "right = spark.read.csv('right.csv',header=True,inferSchema=True)\n",
    "# left.show()\n",
    "# right.show()\n",
    "left.join(right,left.id == right.id,\"inner\").show()\n",
    "# left.join(right,left.id == right.id,\"outer\").show()\n",
    "# left.join(right,left.id == right.id,\"left\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572b94f9",
   "metadata": {},
   "source": [
    "### 6. How to select the recors based on praority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00ca15f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- USING PANDAS-----\n",
      "   id fname    sal sal_source  source_praority\n",
      "0   1     A  11000          X                1\n",
      "2   2     B  14000          X                1\n",
      "4   3     C  15000          Y                1\n",
      "----- USING PYSPARK-----\n",
      "+---+-----+-----+----------+---------------+\n",
      "| id|fname|  sal|sal_source|source_praority|\n",
      "+---+-----+-----+----------+---------------+\n",
      "|  1|    A|11000|         X|              1|\n",
      "|  2|    B|14000|         X|              1|\n",
      "|  3|    C|15000|         Y|              1|\n",
      "+---+-----+-----+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USING SQL\n",
    "# select *,min(praority) from emp group by (fname);\n",
    "# SELECT * FROM (SELECT *, RANK() OVER(PARTITION BY fname ORDER BY praority) AS r FROM emp) sub WHERE r = 1;  \n",
    "\n",
    "print(\"----- USING PANDAS-----\")\n",
    "df = pd.read_csv('test1.csv')\n",
    "print(df[df.source_praority == df.source_praority.min()])\n",
    "\n",
    "print(\"----- USING PYSPARK-----\")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "df = spark.read.csv('test1.csv',header=True,inferSchema=True)\n",
    "\n",
    "# windowDept = Window.partitionBy(\"fname\").orderBy(col(\"source_praority\").desc())\n",
    "windowDept = Window.partitionBy(\"fname\").orderBy(\"source_praority\")\n",
    "\n",
    "df.withColumn(\"row\",row_number().over(windowDept)).filter(col(\"row\") == 1).drop(\"row\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a19062",
   "metadata": {},
   "source": [
    "### 7. how to Canvert Columns to Rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e06c2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- USING PANDAS-----\n",
      "   id fname Country    Exp\n",
      "0   1     A      NA  11000\n",
      "1   2     B      NA  11100\n",
      "2   3     C      NA  11300\n",
      "3   1     A      SA  12000\n",
      "4   2     B      SA  11500\n",
      "5   3     C      SA  12400\n",
      "6   1     A     USA  12500\n",
      "7   2     B     USA  13500\n",
      "8   3     C     USA  12200\n",
      "----- USING PYSPARK-----\n",
      "+---+-----+---+-----+\n",
      "| id|fname|key|  val|\n",
      "+---+-----+---+-----+\n",
      "|  1|    A| NA|11000|\n",
      "|  1|    A| SA|12000|\n",
      "|  1|    A|USA|12500|\n",
      "|  2|    B| NA|11100|\n",
      "|  2|    B| SA|11500|\n",
      "|  2|    B|USA|13500|\n",
      "|  3|    C| NA|11300|\n",
      "|  3|    C| SA|12400|\n",
      "|  3|    C|USA|12200|\n",
      "+---+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USING SQL\n",
    "# SELECT * FROM (\n",
    "#     SELECT id, fname, 'NA' as country, NA as exp FROM emp1\n",
    "#     UNION \n",
    "#     SELECT id, fname,'SA' as country, SA as exp FROM emp1\n",
    "#     UNION \n",
    "#     SELECT id, fname,'USA' as country, USA as exp FROM emp1\n",
    "#     ) AS Example;\n",
    "\n",
    "print(\"----- USING PANDAS-----\")\n",
    "df = pd.read_csv('test2.csv')\n",
    "# df=pd.DataFrame({'id':[1,2,3],'fname':['A','B','C'],'NA':[11000,11100,11300],\n",
    "# 'SA':[12000,11500,12400],'USA':[12500,13500,12200]})\n",
    "\n",
    "print(df.melt(id_vars=[\"id\", \"fname\"], \n",
    "        var_name=\"Country\", \n",
    "        value_name=\"Exp\"))\n",
    "\n",
    "print(\"----- USING PYSPARK-----\")\n",
    "import pyspark.sql.functions as F\n",
    "df = spark.read.csv('test2.csv',header=True,inferSchema=True)\n",
    "\n",
    "def to_long(df, by):\n",
    "\n",
    "    # Filter dtypes and split into column names and type description\n",
    "    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))\n",
    "    # Spark SQL supports only homogeneous columns\n",
    "    assert len(set(dtypes)) == 1, \"All columns have to be of the same type\"\n",
    "\n",
    "    # Create and explode an array of (column_name, column_value) structs\n",
    "    kvs = F.explode(F.array([\n",
    "      F.struct(F.lit(c).alias(\"key\"), F.col(c).alias(\"val\")) for c in cols\n",
    "    ])).alias(\"kvs\")\n",
    "\n",
    "    return df.select(by + [kvs]).select(by + [\"kvs.key\", \"kvs.val\"])\n",
    "\n",
    "outDF = to_long(df, [\"id\",\"fname\"])\n",
    "outDF.show()\n",
    "\n",
    "\n",
    "\n",
    "# df = df.withColumn('label1', F.array(F.lit('NA'), F.col('NA')))\n",
    "# df = df.withColumn('label2', F.array(F.lit('SA'), F.col('SA')))\n",
    "# df = df.withColumn('label3', F.array(F.lit('USA'), F.col('USA')))\n",
    "# # df.show()\n",
    "\n",
    "# # Create a new column which combines the labeled values in a single column\n",
    "# df = df.withColumn('labels', F.array('label1', 'label2', 'label3'))\n",
    "# # df.show()\n",
    "\n",
    "# # Split into individual rows\n",
    "# df = df.withColumn('labels', F.explode('labels'))\n",
    "# # df.show()\n",
    "\n",
    "# # You can now do whatever you want with your labelled rows, eg. split them into new columns\n",
    "# df = df.withColumn('Country', F.col('labels')[0])\n",
    "# df = df.withColumn('EXP', F.col('labels')[1])\n",
    "# # df.show()\n",
    "# df.select(['id','fname','Country','Exp']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791a148",
   "metadata": {},
   "source": [
    "### 8. how to split string into columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1c0a8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- USING PANDAS-----\n",
      "   id variable    names\n",
      "0   1        0      Tom\n",
      "1   2        0     Ritu\n",
      "2   1        1     Rick\n",
      "3   2        1  Shalini\n",
      "4   1        2    Hardy\n",
      "5   2        2   Anjana\n",
      "----- USING PYSAPRK-----\n",
      "+---+--------------------+---+-------+\n",
      "| id|               names|pos|    val|\n",
      "+---+--------------------+---+-------+\n",
      "|  1|  [Tom, Rick, Hardy]|  0|    Tom|\n",
      "|  1|  [Tom, Rick, Hardy]|  1|   Rick|\n",
      "|  1|  [Tom, Rick, Hardy]|  2|  Hardy|\n",
      "|  2|[Ritu, Shalini, A...|  0|   Ritu|\n",
      "|  2|[Ritu, Shalini, A...|  1|Shalini|\n",
      "|  2|[Ritu, Shalini, A...|  2| Anjana|\n",
      "+---+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"----- USING PANDAS-----\")\n",
    "data = {'id': [1, 2],'names': [\"Tom,Rick,Hardy\", \"Ritu,Shalini,Anjana\"]}\n",
    "df = pd.DataFrame(data)\n",
    "# print(df)\n",
    "df_melt = df.assign(names=df.names.str.split(\",\"))\n",
    "# print(df_melt)\n",
    "# df_lt = df_melt.names.apply(pd.Series)\n",
    "# print(df_lt)\n",
    "# print(df_melt.names.apply(pd.Series).merge(df_melt, right_index = True, left_index = True))\n",
    "print(df_melt.names.apply(pd.Series) \\\n",
    "   .merge(df_melt, right_index = True, left_index = True) \\\n",
    "   .drop([\"names\"], axis = 1) \\\n",
    "   .melt(id_vars = ['id'], value_name = \"names\"))\n",
    "\n",
    "print(\"----- USING PYSAPRK-----\")\n",
    "\n",
    "df = spark.read.csv('test3.csv',header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# split_col = f.split(df['names'], ',')\n",
    "# df = df.withColumn('NAME1', split_col.getItem(0))\n",
    "# df = df.withColumn('NAME2', split_col.getItem(1))\n",
    "# df = df.withColumn('NAME3', split_col.getItem(2))\n",
    "# df.show() \n",
    "\n",
    "df.select(\n",
    "        \"id\",\n",
    "        f.split(\"names\", \",\").alias(\"names\"),\n",
    "        f.posexplode(f.split(\"names\", \",\")).alias(\"pos\", \"val\")\n",
    "    ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a46444",
   "metadata": {},
   "source": [
    "### 9. how to find diff in current record and privius record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ddf63e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- USING PANDAS-----\n",
      "   id fname    sal sal_source  source_praority    shift    diff\n",
      "0   1     A  11000          X                1      NaN     NaN\n",
      "1   1     A  12000          Y                2  11000.0  1000.0\n",
      "2   2     B  14000          X                1  12000.0  2000.0\n",
      "3   2     B  13000          Y                2  14000.0 -1000.0\n",
      "4   3     C  15000          Y                1  13000.0  2000.0\n",
      "----- USING PYSPARK-----\n",
      "+---+-----+-----+----------+---------------+-----+----+\n",
      "| id|fname|  sal|sal_source|source_praority|  lag|diff|\n",
      "+---+-----+-----+----------+---------------+-----+----+\n",
      "|  1|    A|11000|         X|              1| null|null|\n",
      "|  1|    A|12000|         Y|              2|11000|1000|\n",
      "|  2|    B|13000|         Y|              2|12000|1000|\n",
      "|  2|    B|14000|         X|              1|13000|1000|\n",
      "|  3|    C|15000|         Y|              1|14000|1000|\n",
      "+---+-----+-----+----------+---------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USING SQL\n",
    "# SELECT id,fname,sal - LAG(sal, 1) OVER (ORDER BY id) FROM emp\n",
    "\n",
    "\n",
    "print(\"----- USING PANDAS-----\")\n",
    "df = pd.read_csv('test1.csv')\n",
    "# df.sort_values(['id','sal'], ascending=[True,True], inplace=True)\n",
    "# df['shift'] = df.groupby('id')['sal'].shift()\n",
    "df['shift'] = df['sal'].shift(1)\n",
    "df['diff'] = df['sal'] - df['shift']\n",
    "print(df)\n",
    "\n",
    "print(\"----- USING PYSPARK-----\")\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "df = spark.read.csv('test1.csv',header=True,inferSchema=True)\n",
    "\n",
    "windowOrder = Window.orderBy(\"sal\")\n",
    "df = df.withColumn(\"lag\", f.lag(\"sal\", 1).over(windowOrder))\n",
    "df.withColumn(\"diff\",df.sal-df.lag).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee122c6b",
   "metadata": {},
   "source": [
    "### 10. customer['cid','name','city','state'] order['oid','cid','itam','price'] find the custimer from 'Mumbai' who has minimam orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbb70aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- USING PANDAS-----\n",
      "   cid   name    city         state  oid\n",
      "3    4  omkar  mumbai  maharashatra    3\n",
      "----- USING PYSPARK-----\n",
      "+---+-----+------+------------+-----+---+\n",
      "|cid| name|  city|       state|count|row|\n",
      "+---+-----+------+------------+-----+---+\n",
      "|  4|omkar|mumbai|maharashatra|    3|  1|\n",
      "+---+-----+------+------------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----- USING PANDAS-----\")\n",
    "customer = pd.read_csv('customer.csv')\n",
    "order = pd.read_csv('order.csv')\n",
    "group = order[['cid','oid']].groupby('cid').count()\n",
    "# print(group)\n",
    "df = pd.merge(customer,group,on='cid')\n",
    "# print(df)\n",
    "df1 = df[(df['city']== 'mumbai')]\n",
    "df1 = df1[df1['oid']==df1['oid'].min()]\n",
    "print(df1)\n",
    "\n",
    "print(\"----- USING PYSPARK-----\")\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "customer = spark.read.csv('customer.csv',header=True,inferSchema=True)\n",
    "order = spark.read.csv('order.csv',header=True,inferSchema=True)\n",
    "# customer.show()\n",
    "# order.show()\n",
    "o_count = order.groupBy('cid').count()\n",
    "# o_count.show()\n",
    "df = customer.join(o_count,on='cid',how='inner')\n",
    "df = df.filter(df['city'] == 'mumbai')\n",
    "# df.show()\n",
    "# w3 = Window.orderBy(col(\"count\").desc()) #bigest\n",
    "w3 = Window.orderBy('count')               #Lowest\n",
    "df = df.withColumn('row',row_number().over(w3))\n",
    "df.filter(df['row'] == 1).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de715326",
   "metadata": {},
   "source": [
    "### 11. RDDs vs. Dataframes in PySpark\n",
    "#### RDDs\n",
    "- No in-built optimization engine for RDDs. Developers need to write the optimized code themselves.\n",
    "- we need to define the schema manually.\n",
    "- RDD is slower than both Dataframes to perform simple operations like grouping the data.\n",
    "\n",
    "#### Dataframes\n",
    "- It uses a catalyst optimizer for optimization.\n",
    "- It will automatically find out the schema of the dataset.\n",
    "- It provides an easy API to perform aggregation operations. It performs aggregation faster than both RDDs and Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ea9c3",
   "metadata": {},
   "source": [
    "### 12. What are workers, executors, cores in Spark Standalone cluster?\n",
    "\n",
    "DRIVER Program(Spark Contax) <----> Cluster Manager <----> Worker Node[Executer[Task][Cache]]\n",
    "\n",
    "#### DRIVER\n",
    "\n",
    "- The driver is the process where the main method runs. First it converts the user program into tasks and after that it schedules the tasks on the executors.\n",
    "\n",
    "#### EXECUTORS\n",
    "\n",
    "- Executors are worker nodes' processes in charge of running individual tasks in a given Spark job. They are launched at the beginning of a Spark application and typically run for the entire lifetime of an application. Once they have run the task they send the results to the driver. They also provide in-memory storage for RDDs that are cached by user programs through Block Manager.\n",
    "\n",
    "#### APPLICATION EXECUTION FLOW\n",
    "\n",
    "##### when you submit an application to the cluster with spark-submit this is what happens internally:\n",
    "\n",
    "- A standalone application starts and instantiates a SparkContext instance (and it is only then when you can call the application a driver).\n",
    "- The driver program ask for resources to the cluster manager to launch executors.\n",
    "- The cluster manager launches executors.\n",
    "- The driver process runs through the user application. Depending on the actions and transformations over RDDs task are sent to executors.\n",
    "- Executors run the tasks and save the results.\n",
    "- If any worker crashes, its tasks will be sent to different executors to be processed again. In the book \"Learning Spark: Lightning-Fast Big Data Analysis\" they talk about Spark and Fault Tolerance:\n",
    "- With SparkContext.stop() from the driver or if the main method exits/crashes all the executors will be terminated and the cluster resources will be released by the cluster manager.\n",
    "\n",
    "##### options of spark-submit:\n",
    "- ./bin/spark-submit [--master \\<master-url>  --deploy-mode \\<deploy-mode> --conf <key<=\\<value>  --driver-memory \\<value>g --executor-memory \\<value>g --executor-cores \\<number of cores> --jars  \\<comma separated dependencies> --class \\<main-class>]\n",
    "\n",
    "##### exampal of spark-submit:\n",
    "./bin/spark-submit --master yarn --deploy-mode cluster wordByExample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe7899",
   "metadata": {},
   "source": [
    "### 13. find third higest salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a178b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select * from(select ename, sal, dense_rank() over(order by sal desc)r from Employee) where r=3;\n",
    "# select * from(select ename, sal, dense_rank() over(partition by department order by sal desc)r from Employee) where r=3;\n",
    "\n",
    "byDeptOrderByAssetDesc = Window.partitionBy(\"department\").orderBy(\"assetValue\" desc)\n",
    "df.withColumn(\"col3\", dense_rank().over(byDeptOrderByAssetDesc)).filter(\"col3=3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
