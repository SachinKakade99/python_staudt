{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b158631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed06305",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pyspark_window\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99ab62c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampleData = ((\"Ram\", 28, \"Sales\", 3000),\n",
    "#               (\"Meena\", 33, \"Sales\", 4600),\n",
    "#               (\"Robin\", 40, \"Sales\", 4100),\n",
    "#               (\"Kunal\", 25, \"Finance\", 3000),\n",
    "#               (\"Ram\", 28, \"Sales\", 3000),\n",
    "#               (\"Srishti\", 46, \"Management\", 3300),\n",
    "#               (\"Jeny\", 26, \"Finance\", 3900),\n",
    "#               (\"Hitesh\", 30, \"Marketing\", 3000),\n",
    "#               (\"Kailash\", 29, \"Marketing\", 2000),\n",
    "#               (\"Sharad\", 39, \"Sales\", 4100)\n",
    "#               )\n",
    "\n",
    "# columns = [\"Employee_Name\", \"Age\",\n",
    "#            \"Department\", \"Salary\"]\n",
    "\n",
    "# df = spark.createDataFrame(data=sampleData,\n",
    "#                            schema=columns)\n",
    "\n",
    "df = spark.read.csv('test4.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f40ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee_Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n",
      "+-------------+---+----------+------+\n",
      "|Employee_Name|Age|Department|Salary|\n",
      "+-------------+---+----------+------+\n",
      "|          Ram| 28|     Sales|  3000|\n",
      "|        Meena| 33|     Sales|  4600|\n",
      "|        Robin| 40|     Sales|  4100|\n",
      "|        Kunal| 25|   Finance|  3000|\n",
      "|          Ram| 28|     Sales|  3000|\n",
      "|      Srishti| 46|Management|  3300|\n",
      "|         Jeny| 26|   Finance|  3900|\n",
      "|       Hitesh| 30| Marketing|  3000|\n",
      "|      Kailash| 29| Marketing|  2000|\n",
      "|       Sharad| 39|     Sales|  4100|\n",
      "+-------------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "309da696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a window\n",
    "# partition of dataframe\n",
    "windowPartition = Window.partitionBy(\"Department\").orderBy(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ea0cccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+---------+\n",
      "|Employee_Name|Age|Department|Salary|cume_dist|\n",
      "+-------------+---+----------+------+---------+\n",
      "|        Kunal| 25|   Finance|  3000|      0.5|\n",
      "|         Jeny| 26|   Finance|  3900|      1.0|\n",
      "|      Srishti| 46|Management|  3300|      1.0|\n",
      "|      Kailash| 29| Marketing|  2000|      0.5|\n",
      "|       Hitesh| 30| Marketing|  3000|      1.0|\n",
      "|          Ram| 28|     Sales|  3000|      0.4|\n",
      "|          Ram| 28|     Sales|  3000|      0.4|\n",
      "|        Meena| 33|     Sales|  4600|      0.6|\n",
      "|       Sharad| 39|     Sales|  4100|      0.8|\n",
      "|        Robin| 40|     Sales|  4100|      1.0|\n",
      "+-------------+---+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "# applying window function with\n",
    "# cume_dist() window function is used to get the cumulative distribution within a window partition. \n",
    "df.withColumn(\"cume_dist\",\n",
    "              cume_dist().over(windowPartition)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff476fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+----+\n",
      "|Employee_Name|Age|Department|Salary| Lag|\n",
      "+-------------+---+----------+------+----+\n",
      "|        Kunal| 25|   Finance|  3000|null|\n",
      "|         Jeny| 26|   Finance|  3900|null|\n",
      "|      Srishti| 46|Management|  3300|null|\n",
      "|      Kailash| 29| Marketing|  2000|null|\n",
      "|       Hitesh| 30| Marketing|  3000|null|\n",
      "|          Ram| 28|     Sales|  3000|null|\n",
      "|          Ram| 28|     Sales|  3000|null|\n",
      "|        Meena| 33|     Sales|  4600|3000|\n",
      "|       Sharad| 39|     Sales|  4100|3000|\n",
      "|        Robin| 40|     Sales|  4100|4600|\n",
      "+-------------+---+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag,when,isnull\n",
    "# A lag() function is used to access previous rowsâ€™ data as per the defined offset value in the function. \n",
    "\n",
    "df.withColumn(\"Lag\", lag(\"Salary\", 2).over(windowPartition)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "552d59c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+----+\n",
      "|Employee_Name|Age|Department|Salary| Lag|\n",
      "+-------------+---+----------+------+----+\n",
      "|        Kunal| 25|   Finance|  3000|null|\n",
      "|         Jeny| 26|   Finance|  3900|3000|\n",
      "|          Ram| 28|     Sales|  3000|3900|\n",
      "|          Ram| 28|     Sales|  3000|3000|\n",
      "|      Kailash| 29| Marketing|  2000|3000|\n",
      "|       Hitesh| 30| Marketing|  3000|2000|\n",
      "|        Meena| 33|     Sales|  4600|3000|\n",
      "|       Sharad| 39|     Sales|  4100|4600|\n",
      "|        Robin| 40|     Sales|  4100|4100|\n",
      "|      Srishti| 46|Management|  3300|4100|\n",
      "+-------------+---+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowOrder = Window.orderBy(\"Age\")\n",
    "df_op = df.withColumn(\"Lag\", lag(\"Salary\", 1).over(windowOrder))\n",
    "df_op.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7006faca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+----+-----+\n",
      "|Employee_Name|Age|Department|Salary| Lag| diff|\n",
      "+-------------+---+----------+------+----+-----+\n",
      "|        Kunal| 25|   Finance|  3000|null| null|\n",
      "|         Jeny| 26|   Finance|  3900|3000|  900|\n",
      "|          Ram| 28|     Sales|  3000|3900| -900|\n",
      "|          Ram| 28|     Sales|  3000|3000|    0|\n",
      "|      Kailash| 29| Marketing|  2000|3000|-1000|\n",
      "|       Hitesh| 30| Marketing|  3000|2000| 1000|\n",
      "|        Meena| 33|     Sales|  4600|3000| 1600|\n",
      "|       Sharad| 39|     Sales|  4100|4600| -500|\n",
      "|        Robin| 40|     Sales|  4100|4100|    0|\n",
      "|      Srishti| 46|Management|  3300|4100| -800|\n",
      "+-------------+---+----------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_op.withColumn(\"diff\",df_op.Salary-df_op.Lag).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfbeb9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+-------+-----+\n",
      "|Employee_Name|Age|Department|Salary|lag_sal| diff|\n",
      "+-------------+---+----------+------+-------+-----+\n",
      "|        Kunal| 25|   Finance|  3000|      0| 3000|\n",
      "|         Jeny| 26|   Finance|  3900|   3000|  900|\n",
      "|          Ram| 28|     Sales|  3000|   3900| -900|\n",
      "|          Ram| 28|     Sales|  3000|   3000|    0|\n",
      "|      Kailash| 29| Marketing|  2000|   3000|-1000|\n",
      "|       Hitesh| 30| Marketing|  3000|   2000| 1000|\n",
      "|        Meena| 33|     Sales|  4600|   3000| 1600|\n",
      "|       Sharad| 39|     Sales|  4100|   4600| -500|\n",
      "|        Robin| 40|     Sales|  4100|   4100|    0|\n",
      "|      Srishti| 46|Management|  3300|   4100| -800|\n",
      "+-------------+---+----------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wind = Window.orderBy(\"Age\")\n",
    "# df_ =  df.withColumn(\"lag_sal\", lag(\"Salary\", 1).over(wind))\n",
    "df_ = df.withColumn(\"lag_sal\", lag(\"Salary\", 1).over(wind)).na.fill(0, ['lag_sal'])\n",
    "\n",
    "df_ = df_.withColumn(\"diff\", df_.Salary - df_.lag_sal)\n",
    "df_.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09382594",
   "metadata": {},
   "source": [
    "### Ranking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c053071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv('test5.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3ff021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Roll_No: integer (nullable = true)\n",
      " |-- Student_Name: string (nullable = true)\n",
      " |-- Subject: string (nullable = true)\n",
      " |-- Marks: integer (nullable = true)\n",
      "\n",
      "+-------+------------+--------------+-----+\n",
      "|Roll_No|Student_Name|       Subject|Marks|\n",
      "+-------+------------+--------------+-----+\n",
      "|    101|         Ram|       Biology|   80|\n",
      "|    103|       Meena|Social Science|   78|\n",
      "|    104|       Robin|      Sanskrit|   58|\n",
      "|    102|       Kunal|       Phisycs|   89|\n",
      "|    101|         Ram|       Biology|   80|\n",
      "|    106|     Srishti|         Maths|   70|\n",
      "|    108|        Jeny|       Physics|   75|\n",
      "|    107|      Hitesh|         Maths|   70|\n",
      "|    109|     Kailash|         Maths|   90|\n",
      "|    105|      Sharad|Social Science|   84|\n",
      "+-------+------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1c4b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a window partition of dataframe\n",
    "windowPartition2 = Window.partitionBy(\"Subject\").orderBy(\"Marks\")\n",
    "# windowPartition2 = Window.partitionBy(\"Subject\").orderBy(\"Marks\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a169cc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+----------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|row_number|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "|    101|         Ram|       Biology|   80|         1|\n",
      "|    101|         Ram|       Biology|   80|         2|\n",
      "|    106|     Srishti|         Maths|   70|         1|\n",
      "|    107|      Hitesh|         Maths|   70|         2|\n",
      "|    109|     Kailash|         Maths|   90|         3|\n",
      "|    102|       Kunal|       Phisycs|   89|         1|\n",
      "|    108|        Jeny|       Physics|   75|         1|\n",
      "|    104|       Robin|      Sanskrit|   58|         1|\n",
      "|    103|       Meena|Social Science|   78|         1|\n",
      "|    105|      Sharad|Social Science|   84|         2|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    " \n",
    "# row_number() function is used to gives a sequential number to each row present in the table.\n",
    "df2.withColumn(\"row_number\",\n",
    "               row_number().over(windowPartition2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba217b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+----+\n",
      "|Roll_No|Student_Name|       Subject|Marks|rank|\n",
      "+-------+------------+--------------+-----+----+\n",
      "|    101|         Ram|       Biology|   80|   1|\n",
      "|    101|         Ram|       Biology|   80|   1|\n",
      "|    106|     Srishti|         Maths|   70|   1|\n",
      "|    107|      Hitesh|         Maths|   70|   1|\n",
      "|    109|     Kailash|         Maths|   90|   3|\n",
      "|    102|       Kunal|       Phisycs|   89|   1|\n",
      "|    108|        Jeny|       Physics|   75|   1|\n",
      "|    104|       Robin|      Sanskrit|   58|   1|\n",
      "|    103|       Meena|Social Science|   78|   1|\n",
      "|    105|      Sharad|Social Science|   84|   2|\n",
      "+-------+------------+--------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# The rank function is used to give ranks to rows specified in the window partition.\n",
    "# This function leaves gaps in rank if there are ties\n",
    "df2.withColumn(\"rank\", rank().over(windowPartition2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5387f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+----------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|dense_rank|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "|    101|         Ram|       Biology|   80|         1|\n",
      "|    101|         Ram|       Biology|   80|         1|\n",
      "|    106|     Srishti|         Maths|   70|         1|\n",
      "|    107|      Hitesh|         Maths|   70|         1|\n",
      "|    109|     Kailash|         Maths|   90|         2|\n",
      "|    102|       Kunal|       Phisycs|   89|         1|\n",
      "|    108|        Jeny|       Physics|   75|         1|\n",
      "|    104|       Robin|      Sanskrit|   58|         1|\n",
      "|    103|       Meena|Social Science|   78|         1|\n",
      "|    105|      Sharad|Social Science|   84|         2|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    " \n",
    "# This function is used to get the rank of each row in the form of row numbers. \n",
    "# This is similar to rank() function, \n",
    "# there is only one difference the rank function leaves gaps in rank when there are ties.\n",
    "df2.withColumn(\"dense_rank\",\n",
    "               dense_rank().over(windowPartition2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "187ee12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+------------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|percent_rank|\n",
      "+-------+------------+--------------+-----+------------+\n",
      "|    101|         Ram|       Biology|   80|         0.0|\n",
      "|    101|         Ram|       Biology|   80|         0.0|\n",
      "|    106|     Srishti|         Maths|   70|         0.0|\n",
      "|    107|      Hitesh|         Maths|   70|         0.0|\n",
      "|    109|     Kailash|         Maths|   90|         1.0|\n",
      "|    102|       Kunal|       Phisycs|   89|         0.0|\n",
      "|    108|        Jeny|       Physics|   75|         0.0|\n",
      "|    104|       Robin|      Sanskrit|   58|         0.0|\n",
      "|    103|       Meena|Social Science|   78|         0.0|\n",
      "|    105|      Sharad|Social Science|   84|         1.0|\n",
      "+-------+------------+--------------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    " \n",
    "# This function is similar to rank() function. It also provides rank to rows but in a percentile format\n",
    "df2.withColumn(\"percent_rank\",\n",
    "               percent_rank().over(windowPartition2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a196cee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+------+-----+----+----+\n",
      "|Employee_Name|Age|Department|Salary|   Avg|  Sum| Min| Max|\n",
      "+-------------+---+----------+------+------+-----+----+----+\n",
      "|        Kunal| 25|   Finance|  3000|3450.0| 6900|3000|3900|\n",
      "|         Jeny| 26|   Finance|  3900|3450.0| 6900|3000|3900|\n",
      "|      Srishti| 46|Management|  3300|3300.0| 3300|3300|3300|\n",
      "|       Hitesh| 30| Marketing|  3000|2500.0| 5000|2000|3000|\n",
      "|      Kailash| 29| Marketing|  2000|2500.0| 5000|2000|3000|\n",
      "|          Ram| 28|     Sales|  3000|3760.0|18800|3000|4600|\n",
      "|        Meena| 33|     Sales|  4600|3760.0|18800|3000|4600|\n",
      "|        Robin| 40|     Sales|  4100|3760.0|18800|3000|4600|\n",
      "|          Ram| 28|     Sales|  3000|3760.0|18800|3000|4600|\n",
      "|       Sharad| 39|     Sales|  4100|3760.0|18800|3000|4600|\n",
      "+-------------+---+----------+------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
    " \n",
    "# creating a window partition of dataframe\n",
    "windowPartitionAgg  = Window.partitionBy(\"Department\")\n",
    " \n",
    "# applying window aggregate function\n",
    "# to df3 with the help of withColumn\n",
    " \n",
    "# this is average()\n",
    "df.withColumn(\"Avg\",avg(col(\"salary\")).over(windowPartitionAgg))\\\n",
    ".withColumn(\"Sum\",sum(col(\"salary\")).over(windowPartitionAgg))\\\n",
    ".withColumn(\"Min\",min(col(\"salary\")).over(windowPartitionAgg))\\\n",
    ".withColumn(\"Max\",max(col(\"salary\")).over(windowPartitionAgg))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ea746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# data1 = {'date': {0: '2018-04-03', 1: '2018-04-04', 2: '2018-04-05', 3: '2018-04-06', 4: '2018-04-07'},\n",
    "#          'id': {0: 'id1', 1: 'id2', 2: 'id1', 3: 'id3', 4: 'id2'},\n",
    "#          'group': {0: '1', 1: '1', 2: '1', 3: '2', 4: '1'},\n",
    "#          'amount': {0: 50, 1: 40, 2: 50, 3: 55, 4: 20}}\n",
    "# df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "# df1_pd.to_csv('test6.csv')\n",
    "\n",
    "# df1 = spark.createDataFrame(df1_pd)\n",
    "# df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da90347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+-----+------+\n",
      "|ID0|      date|id2|group|amount|\n",
      "+---+----------+---+-----+------+\n",
      "|  0|03-04-2018|id1|    1|    50|\n",
      "|  1|04-04-2018|id2|    1|    40|\n",
      "|  2|05-04-2018|id1|    1|    50|\n",
      "|  3|06-04-2018|id3|    2|    55|\n",
      "|  4|07-04-2018|id2|    1|    20|\n",
      "+---+----------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.csv(\"test6.csv\",header=True,inferSchema=True)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb8464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
