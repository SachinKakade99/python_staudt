{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b158631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ed06305",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pyspark_window\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99ab62c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampleData = ((\"Ram\", 28, \"Sales\", 3000),\n",
    "#               (\"Meena\", 33, \"Sales\", 4600),\n",
    "#               (\"Robin\", 40, \"Sales\", 4100),\n",
    "#               (\"Kunal\", 25, \"Finance\", 3000),\n",
    "#               (\"Ram\", 28, \"Sales\", 3000),\n",
    "#               (\"Srishti\", 46, \"Management\", 3300),\n",
    "#               (\"Jeny\", 26, \"Finance\", 3900),\n",
    "#               (\"Hitesh\", 30, \"Marketing\", 3000),\n",
    "#               (\"Kailash\", 29, \"Marketing\", 2000),\n",
    "#               (\"Sharad\", 39, \"Sales\", 4100)\n",
    "#               )\n",
    "\n",
    "# columns = [\"Employee_Name\", \"Age\",\n",
    "#            \"Department\", \"Salary\"]\n",
    "\n",
    "# df = spark.createDataFrame(data=sampleData,\n",
    "#                            schema=columns)\n",
    "\n",
    "df = spark.read.csv('test4.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f40ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee_Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n",
      "+-------------+---+----------+------+\n",
      "|Employee_Name|Age|Department|Salary|\n",
      "+-------------+---+----------+------+\n",
      "|          Ram| 28|     Sales|  3000|\n",
      "|        Meena| 33|     Sales|  4600|\n",
      "|        Robin| 40|     Sales|  4100|\n",
      "|        Kunal| 25|   Finance|  3000|\n",
      "|          Ram| 28|     Sales|  3000|\n",
      "|      Srishti| 46|Management|  3300|\n",
      "|         Jeny| 26|   Finance|  3900|\n",
      "|       Hitesh| 30| Marketing|  3000|\n",
      "|      Kailash| 29| Marketing|  2000|\n",
      "|       Sharad| 39|     Sales|  4100|\n",
      "+-------------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "309da696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a window\n",
    "# partition of dataframe\n",
    "windowPartition = Window.partitionBy(\"Department\").orderBy(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ea0cccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+---------+\n",
      "|Employee_Name|Age|Department|Salary|cume_dist|\n",
      "+-------------+---+----------+------+---------+\n",
      "|        Kunal| 25|   Finance|  3000|      0.5|\n",
      "|         Jeny| 26|   Finance|  3900|      1.0|\n",
      "|      Srishti| 46|Management|  3300|      1.0|\n",
      "|      Kailash| 29| Marketing|  2000|      0.5|\n",
      "|       Hitesh| 30| Marketing|  3000|      1.0|\n",
      "|          Ram| 28|     Sales|  3000|      0.4|\n",
      "|          Ram| 28|     Sales|  3000|      0.4|\n",
      "|        Meena| 33|     Sales|  4600|      0.6|\n",
      "|       Sharad| 39|     Sales|  4100|      0.8|\n",
      "|        Robin| 40|     Sales|  4100|      1.0|\n",
      "+-------------+---+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "# applying window function with\n",
    "# cume_dist() window function is used to get the cumulative distribution within a window partition. \n",
    "df.withColumn(\"cume_dist\",\n",
    "              cume_dist().over(windowPartition)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff476fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+----+\n",
      "|Employee_Name|Age|Department|Salary| Lag|\n",
      "+-------------+---+----------+------+----+\n",
      "|        Kunal| 25|   Finance|  3000|null|\n",
      "|         Jeny| 26|   Finance|  3900|null|\n",
      "|      Srishti| 46|Management|  3300|null|\n",
      "|      Kailash| 29| Marketing|  2000|null|\n",
      "|       Hitesh| 30| Marketing|  3000|null|\n",
      "|          Ram| 28|     Sales|  3000|null|\n",
      "|          Ram| 28|     Sales|  3000|null|\n",
      "|        Meena| 33|     Sales|  4600|3000|\n",
      "|       Sharad| 39|     Sales|  4100|3000|\n",
      "|        Robin| 40|     Sales|  4100|4600|\n",
      "+-------------+---+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag,when,isnull\n",
    "# A lag() function is used to access previous rowsâ€™ data as per the defined offset value in the function. \n",
    "\n",
    "df.withColumn(\"Lag\", lag(\"Salary\", 2).over(windowPartition)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cfbeb9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+-------+-----+\n",
      "|Employee_Name|Age|Department|Salary|lag_sal| diff|\n",
      "+-------------+---+----------+------+-------+-----+\n",
      "|        Kunal| 25|   Finance|  3000|      0| 3000|\n",
      "|         Jeny| 26|   Finance|  3900|   3000|  900|\n",
      "|          Ram| 28|     Sales|  3000|   3900| -900|\n",
      "|          Ram| 28|     Sales|  3000|   3000|    0|\n",
      "|      Kailash| 29| Marketing|  2000|   3000|-1000|\n",
      "|       Hitesh| 30| Marketing|  3000|   2000| 1000|\n",
      "|        Meena| 33|     Sales|  4600|   3000| 1600|\n",
      "|       Sharad| 39|     Sales|  4100|   4600| -500|\n",
      "|        Robin| 40|     Sales|  4100|   4100|    0|\n",
      "|      Srishti| 46|Management|  3300|   4100| -800|\n",
      "+-------------+---+----------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wind = Window.orderBy(\"Age\")\n",
    "# df_ =  df.withColumn(\"lag_sal\", lag(\"Salary\", 1).over(wind))\n",
    "df_ = df.withColumn(\"lag_sal\", lag(\"Salary\", 1).over(wind)).na.fill(0, ['lag_sal'])\n",
    "\n",
    "df_ = df_.withColumn(\"diff\", df_.Salary - df_.lag_sal)\n",
    "df_.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09382594",
   "metadata": {},
   "source": [
    "### Ranking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c053071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv('test5.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3ff021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Roll_No: integer (nullable = true)\n",
      " |-- Student_Name: string (nullable = true)\n",
      " |-- Subject: string (nullable = true)\n",
      " |-- Marks: integer (nullable = true)\n",
      "\n",
      "+-------+------------+--------------+-----+\n",
      "|Roll_No|Student_Name|       Subject|Marks|\n",
      "+-------+------------+--------------+-----+\n",
      "|    101|         Ram|       Biology|   80|\n",
      "|    103|       Meena|Social Science|   78|\n",
      "|    104|       Robin|      Sanskrit|   58|\n",
      "|    102|       Kunal|       Phisycs|   89|\n",
      "|    101|         Ram|       Biology|   80|\n",
      "|    106|     Srishti|         Maths|   70|\n",
      "|    108|        Jeny|       Physics|   75|\n",
      "|    107|      Hitesh|         Maths|   70|\n",
      "|    109|     Kailash|         Maths|   90|\n",
      "|    105|      Sharad|Social Science|   84|\n",
      "+-------+------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1c4b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a window partition of dataframe\n",
    "windowPartition2 = Window.partitionBy(\"Subject\").orderBy(\"Marks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a169cc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+----------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|row_number|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "|    101|         Ram|       Biology|   80|         1|\n",
      "|    101|         Ram|       Biology|   80|         2|\n",
      "|    106|     Srishti|         Maths|   70|         1|\n",
      "|    107|      Hitesh|         Maths|   70|         2|\n",
      "|    109|     Kailash|         Maths|   90|         3|\n",
      "|    102|       Kunal|       Phisycs|   89|         1|\n",
      "|    108|        Jeny|       Physics|   75|         1|\n",
      "|    104|       Robin|      Sanskrit|   58|         1|\n",
      "|    103|       Meena|Social Science|   78|         1|\n",
      "|    105|      Sharad|Social Science|   84|         2|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    " \n",
    "# row_number() function is used to gives a sequential number to each row present in the table.\n",
    "df2.withColumn(\"row_number\",\n",
    "               row_number().over(windowPartition2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba217b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+----+\n",
      "|Roll_No|Student_Name|       Subject|Marks|rank|\n",
      "+-------+------------+--------------+-----+----+\n",
      "|    101|         Ram|       Biology|   80|   1|\n",
      "|    101|         Ram|       Biology|   80|   1|\n",
      "|    106|     Srishti|         Maths|   70|   1|\n",
      "|    107|      Hitesh|         Maths|   70|   1|\n",
      "|    109|     Kailash|         Maths|   90|   3|\n",
      "|    102|       Kunal|       Phisycs|   89|   1|\n",
      "|    108|        Jeny|       Physics|   75|   1|\n",
      "|    104|       Robin|      Sanskrit|   58|   1|\n",
      "|    103|       Meena|Social Science|   78|   1|\n",
      "|    105|      Sharad|Social Science|   84|   2|\n",
      "+-------+------------+--------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# The rank function is used to give ranks to rows specified in the window partition.\n",
    "# This function leaves gaps in rank if there are ties\n",
    "df2.withColumn(\"rank\", rank().over(windowPartition2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5387f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+----------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|dense_rank|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "|    101|         Ram|       Biology|   80|         1|\n",
      "|    101|         Ram|       Biology|   80|         1|\n",
      "|    106|     Srishti|         Maths|   70|         1|\n",
      "|    107|      Hitesh|         Maths|   70|         1|\n",
      "|    109|     Kailash|         Maths|   90|         2|\n",
      "|    102|       Kunal|       Phisycs|   89|         1|\n",
      "|    108|        Jeny|       Physics|   75|         1|\n",
      "|    104|       Robin|      Sanskrit|   58|         1|\n",
      "|    103|       Meena|Social Science|   78|         1|\n",
      "|    105|      Sharad|Social Science|   84|         2|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    " \n",
    "# This function is used to get the rank of each row in the form of row numbers. \n",
    "# This is similar to rank() function, \n",
    "# there is only one difference the rank function leaves gaps in rank when there are ties.\n",
    "df2.withColumn(\"dense_rank\",\n",
    "               dense_rank().over(windowPartition2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "187ee12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+------------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|percent_rank|\n",
      "+-------+------------+--------------+-----+------------+\n",
      "|    101|         Ram|       Biology|   80|         0.0|\n",
      "|    101|         Ram|       Biology|   80|         0.0|\n",
      "|    106|     Srishti|         Maths|   70|         0.0|\n",
      "|    107|      Hitesh|         Maths|   70|         0.0|\n",
      "|    109|     Kailash|         Maths|   90|         1.0|\n",
      "|    102|       Kunal|       Phisycs|   89|         0.0|\n",
      "|    108|        Jeny|       Physics|   75|         0.0|\n",
      "|    104|       Robin|      Sanskrit|   58|         0.0|\n",
      "|    103|       Meena|Social Science|   78|         0.0|\n",
      "|    105|      Sharad|Social Science|   84|         1.0|\n",
      "+-------+------------+--------------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    " \n",
    "# This function is similar to rank() function. It also provides rank to rows but in a percentile format\n",
    "df2.withColumn(\"percent_rank\",\n",
    "               percent_rank().over(windowPartition2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a196cee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+------+-----+----+----+\n",
      "|Employee_Name|Age|Department|Salary|   Avg|  Sum| Min| Max|\n",
      "+-------------+---+----------+------+------+-----+----+----+\n",
      "|        Kunal| 25|   Finance|  3000|3450.0| 6900|3000|3900|\n",
      "|         Jeny| 26|   Finance|  3900|3450.0| 6900|3000|3900|\n",
      "|      Srishti| 46|Management|  3300|3300.0| 3300|3300|3300|\n",
      "|       Hitesh| 30| Marketing|  3000|2500.0| 5000|2000|3000|\n",
      "|      Kailash| 29| Marketing|  2000|2500.0| 5000|2000|3000|\n",
      "|          Ram| 28|     Sales|  3000|3760.0|18800|3000|4600|\n",
      "|        Meena| 33|     Sales|  4600|3760.0|18800|3000|4600|\n",
      "|        Robin| 40|     Sales|  4100|3760.0|18800|3000|4600|\n",
      "|          Ram| 28|     Sales|  3000|3760.0|18800|3000|4600|\n",
      "|       Sharad| 39|     Sales|  4100|3760.0|18800|3000|4600|\n",
      "+-------------+---+----------+------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
    " \n",
    "# creating a window partition of dataframe\n",
    "windowPartitionAgg  = Window.partitionBy(\"Department\")\n",
    " \n",
    "# applying window aggregate function\n",
    "# to df3 with the help of withColumn\n",
    " \n",
    "# this is average()\n",
    "df.withColumn(\"Avg\",avg(col(\"salary\")).over(windowPartitionAgg)).withColumn(\"Sum\",sum(col(\"salary\")).over(windowPartitionAgg)).withColumn(\"Min\",min(col(\"salary\")).over(windowPartitionAgg)).withColumn(\"Max\",max(col(\"salary\")).over(windowPartitionAgg)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ea746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# data1 = {'date': {0: '2018-04-03', 1: '2018-04-04', 2: '2018-04-05', 3: '2018-04-06', 4: '2018-04-07'},\n",
    "#          'id': {0: 'id1', 1: 'id2', 2: 'id1', 3: 'id3', 4: 'id2'},\n",
    "#          'group': {0: '1', 1: '1', 2: '1', 3: '2', 4: '1'},\n",
    "#          'amount': {0: 50, 1: 40, 2: 50, 3: 55, 4: 20}}\n",
    "# df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "# df1_pd.to_csv('test6.csv')\n",
    "\n",
    "# df1 = spark.createDataFrame(df1_pd)\n",
    "# df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da90347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+-----+------+\n",
      "|ID0|      date|id2|group|amount|\n",
      "+---+----------+---+-----+------+\n",
      "|  0|03-04-2018|id1|    1|    50|\n",
      "|  1|04-04-2018|id2|    1|    40|\n",
      "|  2|05-04-2018|id1|    1|    50|\n",
      "|  3|06-04-2018|id3|    2|    55|\n",
      "|  4|07-04-2018|id2|    1|    20|\n",
      "+---+----------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.csv(\"test6.csv\",header=True,inferSchema=True)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7859b10",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-77d3e90ca9d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"amount\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# Sort for easy inspection. Not necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"group\").orderBy(\"date\").rowsBetween(\n",
    "    Window.unboundedPreceding,  # Take all rows from the beginning of frame\n",
    "    Window.currentRow           # To current row\n",
    ")\n",
    "\n",
    "df3.withColumn(\"sum\", sum(\"amount\").over(w)).orderBy(\"date\").show()   # Sort for easy inspection. Not necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fb87a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a5927f6f2521>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmy_window\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ID0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"prev_value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_window\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m df = df.withColumn(\"diff\", F.when(F.isnull(df.value - df.prev_value), 0)\n\u001b[0;32m      6\u001b[0m                               .otherwise(df.value - df.prev_value))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1657\u001b[0m         \"\"\"\n\u001b[0;32m   1658\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m             raise AttributeError(\n\u001b[0m\u001b[0;32m   1660\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "my_window = Window.partitionBy().orderBy(\"ID0\")\n",
    "\n",
    "df = df.withColumn(\"prev_value\", F.lag(df.value).over(my_window))\n",
    "df = df.withColumn(\"diff\", F.when(F.isnull(df.value - df.prev_value), 0)\n",
    "                              .otherwise(df.value - df.prev_value))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891e300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb8464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
=======
 "cells": [],
 "metadata": {},
>>>>>>> origin/home
 "nbformat": 4,
 "nbformat_minor": 5
}
